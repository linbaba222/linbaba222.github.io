<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>理解反向传播算法与python实现 | Auspic</title><meta name="description" content="在这里记录一下神经网络中反向传播算法的实现步骤与理解以及python实现这个算法。  训练一个神经网络算法首先对参数进行初始化，之前的线性回归与逻辑回归算法参数默认初始值都是0，然后通过梯度下降算法来逐步修正参数而这在神经网络算法中是不行的，如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。通常初始参数为正负之间的随机值  利用正向传播算法来计算每一层中所有的激活单"><meta name="author" content="Auspic,2218684601@qq.com"><meta name="copyright" content="Auspic"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://lzxandcyx.cn/2020/12/31/%E7%90%86%E8%A7%A3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%8Epython%E5%AE%9E%E7%8E%B0/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="理解反向传播算法与python实现"><meta property="og:url" content="http://lzxandcyx.cn/2020/12/31/%E7%90%86%E8%A7%A3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%8Epython%E5%AE%9E%E7%8E%B0/"><meta property="og:site_name" content="Auspic"><meta property="og:description" content="在这里记录一下神经网络中反向传播算法的实现步骤与理解以及python实现这个算法。  训练一个神经网络算法首先对参数进行初始化，之前的线性回归与逻辑回归算法参数默认初始值都是0，然后通过梯度下降算法来逐步修正参数而这在神经网络算法中是不行的，如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。通常初始参数为正负之间的随机值  利用正向传播算法来计算每一层中所有的激活单"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2020-12-31T01:47:12.000Z"><meta property="article:modified_time" content="2020-12-31T03:30:42.412Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="prev" title="RDD的操作特性与转换-行动操作" href="http://lzxandcyx.cn/2020/12/31/RDD%E7%9A%84%E6%93%8D%E4%BD%9C%E7%89%B9%E6%80%A7%E4%B8%8E%E8%BD%AC%E6%8D%A2-%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C/"><link rel="next" title="docker容器常用命令" href="http://lzxandcyx.cn/2020/12/30/docker%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 5.0.0"></head><body><canvas class="fireworks"></canvas><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">114</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 杂</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 常用网站</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#python%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.</span> <span class="toc-text">python实现</span></a></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Auspic</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 杂</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 常用网站</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">理解反向传播算法与python实现</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-12-31 09:47:12"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-12-31</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-31 11:30:42"><i class="fas fa-history fa-fw"></i> 更新于 2020-12-31</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p><strong>在这里记录一下神经网络中反向传播算法的实现步骤与理解以及python实现这个算法。</strong></p>
<ul>
<li><p>训练一个神经网络算法首先对参数进行初始化，之前的线性回归与逻辑回归算法参数默认初始值都是0，然后通过梯度下降算法来逐步修正参数而这在神经网络算法中是不行的，如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。通常初始参数为正负<img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231100340988.png" alt="image-20201231100340988">之间的随机值</p>
</li>
<li><p>利用正向传播算法来计算每一层中所有的激活单元</p>
<p> <img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231100755234.png" alt="image-20201231100755234"></p>
<p> <img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231100725980.png" alt="image-20201231100725980"></p>
</li>
<li><p>写出神经网络算法的代价函数：</p>
<p><img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231100943524.png" alt="image-20201231100943524"></p>
</li>
</ul>
<p>  <strong>注意：代价函数的正则项将隐藏层的数据也会计算到代价函数中去。</strong></p>
<ul>
<li><p>利用反向传播算法计算代价函数的偏导数，进行反向传播。</p>
<p>利用反向传播算法计算偏导：</p>
<p><img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231101759076.png" alt="image-20201231101759076"></p>
<p><img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231101343984.png" alt="image-20201231101343984"> 代表目前所计算的是第几层。</p>
<p><img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231101411527.png" alt="image-20201231101411527">代表目前计算层中的激活单元的下标，也将是下一层的第<img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231101411527.png" alt="image-20201231101411527">个输入变量的下标。</p>
<p><strong>i</strong>代表下一层中误差单元的下标，是受到权重矩阵中第<strong>i</strong>行影响的下一层中的误差单元的下标。</p>
<p>用<img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231101635238.png" alt="image-20201231101635238">来表示误差，则：<img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231101833390.png" alt="image-20201231101833390"></p>
</li>
</ul>
<p><img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231101920933.png" alt="image-20201231101920933"></p>
<p><strong>更直观的理解反向传播函数，从右往左计算误差，就相当于上一层的误差等于下一层的误差在与上一层相连的边上的权重之和</strong></p>
<p><img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231102517990.png" alt="image-20201231102517990"></p>
<ul>
<li><p>利用梯度检验来检验这些偏导数</p>
<p><img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231102556216.png" alt="image-20201231102556216"></p>
</li>
</ul>
<p><img src= "/img/loading.gif" data-src="https://gitee.com/AuspicL/auspicbemap/raw/master/imag/image-20201231102624426.png" alt="image-20201231102624426"></p>
<h3 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report<span class="comment">#这个包是评价报告</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>(<span class="params">path, transpose=True</span>):</span></span><br><span class="line">    data = sio.loadmat(path)</span><br><span class="line">    y = data.get(<span class="string">&#x27;y&#x27;</span>)  <span class="comment"># (5000,1)</span></span><br><span class="line">    y = y.reshape(y.shape[<span class="number">0</span>])  <span class="comment"># make it back to column vector</span></span><br><span class="line"></span><br><span class="line">    X = data.get(<span class="string">&#x27;X&#x27;</span>)  <span class="comment"># (5000,400)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> transpose:</span><br><span class="line">        <span class="comment"># for this dataset, you need a transpose to get the orientation right</span></span><br><span class="line">        X = np.array([im.reshape((<span class="number">20</span>, <span class="number">20</span>)).T <span class="keyword">for</span> im <span class="keyword">in</span> X])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># and I flat the image again to preserve the vector presentation</span></span><br><span class="line">        X = np.array([im.reshape(<span class="number">400</span>) <span class="keyword">for</span> im <span class="keyword">in</span> X])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"><span class="comment">#先可视化一下数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_100_image</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; sample 100 image and show them</span></span><br><span class="line"><span class="string">    assume the image is square</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    X : (5000, 400)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    size = int(np.sqrt(X.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sample 100 image, reshape, reorg it</span></span><br><span class="line">    sample_idx = np.random.choice(np.arange(X.shape[<span class="number">0</span>]), <span class="number">100</span>)  <span class="comment"># 100*400</span></span><br><span class="line">    sample_images = X[sample_idx, :]</span><br><span class="line"></span><br><span class="line">    fig, ax_array = plt.subplots(nrows=<span class="number">10</span>, ncols=<span class="number">10</span>, sharey=<span class="literal">True</span>, sharex=<span class="literal">True</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            ax_array[r, c].matshow(sample_images[<span class="number">10</span> * r + c].reshape((size, size)),</span><br><span class="line">                                   cmap=matplotlib.cm.binary)</span><br><span class="line">            plt.xticks(np.array([]))</span><br><span class="line">            plt.yticks(np.array([]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#将y值转换一下</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_y</span>(<span class="params">y</span>):</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;expand 5000*1 into 5000*10</span></span><br><span class="line"><span class="comment">#     where y=10 -&gt; [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]: ndarray</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;</span></span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> y:</span><br><span class="line">        y_array = np.zeros(<span class="number">10</span>)</span><br><span class="line">        y_array[i - <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        res.append(y_array)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(res)</span><br><span class="line"><span class="comment"># from sklearn.preprocessing import OneHotEncoder</span></span><br><span class="line"><span class="comment"># encoder = OneHotEncoder(sparse=False)</span></span><br><span class="line"><span class="comment"># y_onehot = encoder.fit_transform(y)</span></span><br><span class="line"><span class="comment"># y_onehot.shape #这个函数与expand_y(y)一致</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#读取权重</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weight</span>(<span class="params">path</span>):</span></span><br><span class="line">    data = sio.loadmat(path)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">&#x27;Theta1&#x27;</span>], data[<span class="string">&#x27;Theta2&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#将参数扁平化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serialize</span>(<span class="params">a, b</span>):</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> np.concatenate((np.ravel(a), np.ravel(b)))</span><br><span class="line"><span class="comment"># 序列化2矩阵</span></span><br><span class="line"><span class="comment"># 在这个nn架构中，我们有theta1（25,401），theta2（10,26），它们的梯度是delta1，delta2  </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#前向传播函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span>(<span class="params">theta, X</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;apply to architecture 400+1 * 25+1 *10</span></span><br><span class="line"><span class="string">    X: 5000 * 401</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    t1, t2 = deserialize(theta)  <span class="comment"># t1: (25,401) t2: (10,26)</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    a1 = X  <span class="comment"># 5000 * 401</span></span><br><span class="line"></span><br><span class="line">    z2 = a1 @ t1.T  <span class="comment"># 5000 * 25</span></span><br><span class="line">    a2 = np.insert(sigmoid(z2), <span class="number">0</span>, np.ones(m), axis=<span class="number">1</span>)  <span class="comment"># 5000*26</span></span><br><span class="line"></span><br><span class="line">    z3 = a2 @ t2.T  <span class="comment"># 5000 * 10</span></span><br><span class="line">    h = sigmoid(z3)  <span class="comment"># 5000*10, this is h_theta(X)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a1, z2, a2, z3, h  <span class="comment"># you need all those for backprop</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#S型函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="comment">#将扁平化后的参数重新定型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deserialize</span>(<span class="params">seq</span>):</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;into ndarray of (25, 401), (10, 26)&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> seq[:<span class="number">25</span> * <span class="number">401</span>].reshape(<span class="number">25</span>, <span class="number">401</span>), seq[<span class="number">25</span> * <span class="number">401</span>:].reshape(<span class="number">10</span>, <span class="number">26</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">theta, X, y</span>):</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;calculate cost</span></span><br><span class="line"><span class="comment">#     y: (m, k) ndarray</span></span><br><span class="line"><span class="comment">#     &quot;&quot;&quot;</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]  <span class="comment"># get the data size m</span></span><br><span class="line"></span><br><span class="line">    _, _, _, _, h = feed_forward(theta, X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># np.multiply is pairwise operation</span></span><br><span class="line">    pair_computation = -np.multiply(y, np.log(h)) - np.multiply((<span class="number">1</span> - y), np.log(<span class="number">1</span> - h))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pair_computation.sum() / m</span><br><span class="line"></span><br><span class="line"><span class="comment">#正则化代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_cost</span>(<span class="params">theta, X, y, l=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;the first column of t1 and t2 is intercept theta, ignore them when you do regularization&quot;&quot;&quot;</span></span><br><span class="line">    t1, t2 = deserialize(theta)  <span class="comment"># t1: (25,401) t2: (10,26)</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    reg_t1 = (l / (<span class="number">2</span> * m)) * np.power(t1[:, <span class="number">1</span>:], <span class="number">2</span>).sum()  <span class="comment"># this is how you ignore first col</span></span><br><span class="line">    reg_t2 = (l / (<span class="number">2</span> * m)) * np.power(t2[:, <span class="number">1</span>:], <span class="number">2</span>).sum()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost(theta, X, y) + reg_t1 + reg_t2</span><br><span class="line"></span><br><span class="line"><span class="comment">#S型函数求导</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_gradient</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    pairwise op is key for this to work on both vector and matrix</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> np.multiply(sigmoid(z), <span class="number">1</span> - sigmoid(z))</span><br><span class="line"></span><br><span class="line"><span class="comment">#反向传播计算梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">theta, X, y</span>):</span></span><br><span class="line">    <span class="comment"># initialize</span></span><br><span class="line">    t1, t2 = deserialize(theta)  <span class="comment"># t1: (25,401) t2: (10,26)</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    delta1 = np.zeros(t1.shape)  <span class="comment"># (25, 401)</span></span><br><span class="line">    delta2 = np.zeros(t2.shape)  <span class="comment"># (10, 26)</span></span><br><span class="line"></span><br><span class="line">    a1, z2, a2, z3, h = feed_forward(theta, X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        a1i = a1[i, :]  <span class="comment"># (1, 401)</span></span><br><span class="line">        z2i = z2[i, :]  <span class="comment"># (1, 25)</span></span><br><span class="line">        a2i = a2[i, :]  <span class="comment"># (1, 26)</span></span><br><span class="line"></span><br><span class="line">        hi = h[i, :]    <span class="comment"># (1, 10)</span></span><br><span class="line">        yi = y[i, :]    <span class="comment"># (1, 10)</span></span><br><span class="line"></span><br><span class="line">        d3i = hi - yi  <span class="comment"># (1, 10)</span></span><br><span class="line"></span><br><span class="line">        z2i = np.insert(z2i, <span class="number">0</span>, np.ones(<span class="number">1</span>))  <span class="comment"># make it (1, 26) to compute d2i</span></span><br><span class="line">        d2i = np.multiply(t2.T @ d3i, sigmoid_gradient(z2i))  <span class="comment"># (1, 26)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># careful with np vector transpose</span></span><br><span class="line">        delta2 += np.matrix(d3i).T @ np.matrix(a2i)  <span class="comment"># (1, 10).T @ (1, 26) -&gt; (10, 26)</span></span><br><span class="line">        delta1 += np.matrix(d2i[<span class="number">1</span>:]).T @ np.matrix(a1i)  <span class="comment"># (1, 25).T @ (1, 401) -&gt; (25, 401)</span></span><br><span class="line"></span><br><span class="line">    delta1 = delta1 / m</span><br><span class="line">    delta2 = delta2 / m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> serialize(delta1, delta2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#梯度校验函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_checking</span>(<span class="params">theta, X, y, epsilon, regularized=False</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a_numeric_grad</span>(<span class="params">plus, minus, regularized=False</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;calculate a partial gradient with respect to 1 theta&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> regularized:</span><br><span class="line">            <span class="keyword">return</span> (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (epsilon * <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (cost(plus, X, y) - cost(minus, X, y)) / (epsilon * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    theta_matrix = expand_array(theta)  <span class="comment"># expand to (10285, 10285)</span></span><br><span class="line">    epsilon_matrix = np.identity(len(theta)) * epsilon</span><br><span class="line"></span><br><span class="line">    plus_matrix = theta_matrix + epsilon_matrix</span><br><span class="line">    minus_matrix = theta_matrix - epsilon_matrix</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate numerical gradient with respect to all theta</span></span><br><span class="line">    numeric_grad = np.array([a_numeric_grad(plus_matrix[i], minus_matrix[i], regularized)</span><br><span class="line">                                    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta))])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># analytical grad will depend on if you want it to be regularized or not</span></span><br><span class="line">    analytic_grad = regularized_gradient(theta, X, y) <span class="keyword">if</span> regularized <span class="keyword">else</span> gradient(theta, X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If you have a correct implementation, and assuming you used EPSILON = 0.0001</span></span><br><span class="line">    <span class="comment"># the diff below should be less than 1e-9</span></span><br><span class="line">    <span class="comment"># this is how original matlab code do gradient checking</span></span><br><span class="line">    diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: &#123;&#125;\n&#x27;</span>.format(diff))</span><br><span class="line"></span><br><span class="line"><span class="comment">#正则化梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_gradient</span>(<span class="params">theta, X, y, l=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;don&#x27;t regularize theta of bias terms&quot;&quot;&quot;</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    delta1, delta2 = deserialize(gradient(theta, X, y))</span><br><span class="line">    t1, t2 = deserialize(theta)</span><br><span class="line"></span><br><span class="line">    t1[:, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    reg_term_d1 = (l / m) * t1</span><br><span class="line">    delta1 = delta1 + reg_term_d1</span><br><span class="line"></span><br><span class="line">    t2[:, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    reg_term_d2 = (l / m) * t2</span><br><span class="line">    delta2 = delta2 + reg_term_d2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> serialize(delta1, delta2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#参数随机初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_init</span>(<span class="params">size</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.random.uniform(<span class="number">-0.12</span>, <span class="number">0.12</span>, size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用科学计算库进行最小化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_training</span>(<span class="params">X, y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;regularized version</span></span><br><span class="line"><span class="string">    the architecture is hard coded here... won&#x27;t generalize</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    init_theta = random_init(<span class="number">10285</span>)  <span class="comment"># 25*401 + 10*26</span></span><br><span class="line"></span><br><span class="line">    res = opt.minimize(fun=regularized_cost,</span><br><span class="line">                       x0=init_theta,</span><br><span class="line">                       args=(X, y, <span class="number">1</span>),</span><br><span class="line">                       method=<span class="string">&#x27;TNC&#x27;</span>,</span><br><span class="line">                       jac=regularized_gradient,</span><br><span class="line">                       options=&#123;<span class="string">&#x27;maxiter&#x27;</span>: <span class="number">400</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示一下准确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_accuracy</span>(<span class="params">theta, X, y</span>):</span></span><br><span class="line">    _, _, _, _, h = feed_forward(theta, X)</span><br><span class="line"></span><br><span class="line">    y_pred = np.argmax(h, axis=<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    print(classification_report(y, y_pred))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:2218684601@qq.com">Auspic</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://lzxandcyx.cn/2020/12/31/%E7%90%86%E8%A7%A3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%8Epython%E5%AE%9E%E7%8E%B0/">http://lzxandcyx.cn/2020/12/31/%E7%90%86%E8%A7%A3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%B8%8Epython%E5%AE%9E%E7%8E%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://lzxandcyx.cn" target="_blank">Auspic</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/12/31/RDD%E7%9A%84%E6%93%8D%E4%BD%9C%E7%89%B9%E6%80%A7%E4%B8%8E%E8%BD%AC%E6%8D%A2-%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C/"><img class="prev-cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">RDD的操作特性与转换-行动操作</div></div></a></div><div class="next-post pull-right"><a href="/2020/12/30/docker%E5%AE%B9%E5%99%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"><img class="next-cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">docker容器常用命令</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By Auspic</div><div class="framework-info"><span>驱动 </span><a target="_blank" rel="noopener" href="https://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">繁</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="/js/third-party/fireworks.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script></body></html>